---
title: "Lab 8"
subtitle: ""
author: "Attila Lazar"
date: "02.12.2020"
output: pdf_document
---

# 1)

## data

We load the dataset. We select observations with response *1* or *3*

```{r, echo=TRUE}

#install.packages("rrcov")
data(olitos, package="rrcov")
olitos.a <- olitos[which(olitos$grp %in% c(1,3)), -26]
grp <- olitos[which(olitos$grp %in% c(1,3)), "grp"]
y <- ifelse(grp==1, 1, 0)
olitos.a <- cbind(olitos.a, y)

set.seed(1234)
n <- nrow(olitos.a)
train.a <- sample(1:n, round(n*2/3))
test.a <- (1:n) [-train.a]

```

## a)

We train our model using the training dataset and use only variables *X1* to *X2*

```{r, echo=TRUE}

modelglm <- glm(y~X1+X2+X3+X4+X5+X6, data=olitos.a, family="binomial", subset=train.a)
modelglm
summary(modelglm)

```

*X3* and *X6* and also *X1* seem to be the significatly contributing variables.

## b)

we plot predictions for the test set

```{r, echo=TRUE}

pred.a <- predict(modelglm, olitos.a[test.a,], type="response")
plot(pred.a, col=as.numeric(olitos.a[test.a,"y"]+1))
abline(h=0.5)

```

and calculate the confusion matrix, and the classification error

```{r, echo=TRUE}
T <- table(olitos.a[test.a,"y"], pred.a>0.5)
T
e1 <- 1-sum(diag(T))/sum(T)
e1

```

### c)

Now we train the model with all variables

```{r, echo=TRUE}
modelglm.c <- glm(y~.,data=olitos.a, family="binomial", subset=train.a)
modelglm.c
summary(modelglm.c)

```

Here the inference does not work. we also get a warning, probably becouse we have to few samples. We plot the predictions

```{r, echo=TRUE}
pred.c <- predict(modelglm.c, olitos.a[test.a,], type="response")
plot(pred.c, col=as.numeric(olitos.a[test.a,"y"]+1))
abline(h=0.5)

```

The confusion Matrix shows that we get worse results using all variables.

```{r, echo=TRUE}
T <- table(olitos.a[test.a,"y"], pred.c>0.5)
T
e2 <- 1-sum(diag(T))/sum(T)
e2


```

# 2)

# a)

We compute a model using all response variables and the explanatory variables *X1* to *X6*

```{r, echo=TRUE, warning=FALSE}
#install.packages("VGAM")

n <- nrow(olitos)
train <- sample(1:n, round(n*2/3))
test <- (1:n) [-train]



library(VGAM)
?vglm

modelvglm <- vglm(grp~X1+X2+X3+X4+X5+X6,data=olitos, family="multinomial", subset=train)
summary(modelvglm)

```

According to the inference table none of the variables is significatly contributig

## b)

We compute the confusion matrix and calculate the missclassification rate

```{r, echo=TRUE}
pred.2 <- predict(modelvglm, olitos[test,], type="link")
#plot(pred.2, col=as.numeric(olitos[test,"grp"]))

T <- table(olitos[test,"grp"], apply(pred.2, 1, which.max))
T
e1 <- 1-sum(diag(T))/sum(T)
e1

```

# 3)

## a)

We use the function *cv.glmnet()*

```{r, echo=TRUE, warning=FALSE}

library(glmnet)
?cv.glmnet
X <- as.matrix(olitos[train, -26])
y <- as.numeric(olitos[train, "grp"])-1
modelvglm.3 <- cv.glmnet(x=X, y=y, family="multinomial")
#summary(modelvglm.3)
```

durring training we get the following warning:

## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground


```{r, echo=TRUE}
plot(modelvglm.3)

```

The plot shows us the optimal lambda parameter around log(-4)

## b)

We use this lambda parameter to predict values of the test set anc compute our missclassification rate

```{r, echo=TRUE}

modelvglm.3b <- glmnet(x=X, y=y, family="multinomial")

pred.3b <- predict(modelvglm.3b, as.matrix(olitos[test,-26]), type="link", s=0.0001)

T <- table(olitos[test,"grp"], apply(pred.3b, 1, which.max))
T
e1 <- 1-sum(diag(T))/sum(T)
e1

```

# 4)

## a)

We split or data in train and test sets

```{r, echo=TRUE}
bank <- read.csv2("data/bank.csv")
set.seed(1234)
train <- sample(1:nrow(bank), 3000)
test <- (1:nrow(bank)) [-train]


```

train using *glm* on the train set

```{r, echo=TRUE}

modelglm.4 <- glm(y~.,data=bank, family=binomial, subset=train)
modelglm.4
summary(modelglm.4)

```
We see that some variables are more significant than others. *contactunknown*, *monthoct*, *duration* and *poutcomesuccess* contribute the most

We plot predictions on the test-set.

```{r, echo=TRUE}

pred.4 <- predict(modelglm.4, bank[test,], type="link")
plot(pred.4, col=as.numeric(bank[test, "y"]))
abline(h=0)
abline(h=-2.5)

```

We calculate the confusion table. To minimaze false negatives we shift the decision boundary to -2.5

```{r, echo=TRUE}
T <- table(bank[test,"y"], pred.4>0)
T


T <- table(bank[test,"y"], pred.4>-2.5)
T

```

```{r, echo=TRUE}
#modelglm.4b <- glm(y~.,data=bank, family=binomial, subset=train, weights = seq(1,16))

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```

```{r, echo=TRUE}

```
